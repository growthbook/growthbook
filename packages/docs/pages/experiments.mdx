# Experiments

Experiments are the core of Growth Book.  This page covers several topics:

1.  Creating
2.  Targeting
3.  Implementation
4.  Starting
5.  Stopping

## Creating

When you create a new experiment, it starts out as a draft and remains fully editable until you start it.

Experiment drafts are a great place to collaborate between PMs, designers, and engineers. 

PMs can spec out the requirements, designers can upload mockups and screenshots, and engineers can work on implementation.

Use the built-in discussion thread to add comments or collect feedback.

## Targeting

There are a few different ways to limit your experiment to a subset of users:

1.  Specify a pre-defined **Segment**
2.  Limit to logged-in users only
3.  Specify a URL regex pattern
4.  Custom targeting rules (e.g. `age > 18`)

All of these rules are evaluated locally in your app at runtime and no HTTP requests are made to the GrowthBook servers.

## Implementation

We offer official client libraries in a few popular languages with more coming soon:

*  Javascript/Typescript - <https://github.com/growthbook/growthbook-js>
*  ReactJS - <https://github.com/growthbook/growthbook-react>
*  PHP - <https://github.com/growthbook/growthbook-php>
*  Ruby - <https://github.com/growthbook/growthbook-ruby>
*  Go - *coming soon*
*  Python - *coming soon*

There are 4 different styles of implementation to choose from:

### 1. Visual Designer

For simple experiments, we offer a Visual Designer within GrowthBook where you can modify DOM elements on your page and inject CSS rules.  

This is perfect for when you just want to test a new headline or change button colors.

No code changes are required for this implementation method.

### 2. Code Branching

Branching is best when you need precise control over how the variations behave.

```ts
result = user.experiment("my-test");
if (result.variation === 1) {
  // Variation
  buttonColor = blue
}
else {
  // Control
  buttonColor = green
}
```

### 3. Parameterization

If you attach data to variations, you can simplify your implementation code a bit.

```ts
result = user.experiment("my-test");
buttonColor = result.data.color;
```

### 4. Feature Flags

If you have an existing feature flag system, you can do a deeper integration.

This requires some up-front work to integrate GrowthBook with your feature flag system, but after that there is little to no maintenance required.

After integrating, you simply attach data to variations with the same keys are your feature flags.

```ts
buttonColor = myFeatureFlagSystem.getFlag("homepage.button.color");
```


## Starting

Once you start an experiment, you are unable to modify certain things like the number of variations or the data source.
This is to prevent changes that would invalidate the assumptions of our stats engine.

When you start an experiment, the client libraries should begin assigning people to variations within a few minutes.

### Phases

Experiments go through one or more "phases". For example, it might start with a "ramp" phase to 10% of traffic to detect any major bugs and reduce risk.  Then it would go on to
the main phase at 100% traffic.  And finally, it may have a holdout phase where 10% of traffic continues to get the control to allow for evaluating
long-term metrics.

Phases cannot be modified once they are created. If you want to make changes, you need to start a new phase.

The following fields are available when starting a phase:

-  **Experiment Phase** - The type of phase.  Either `ramp`, `main`, or `holdout`.
-  **Percent of Traffic** - A float from 0 to 1 representing what percent of traffic should be included in the experiment.
-  **Traffic Split** - Floats that must add up to 1 and represent the split between the variations.
-  **Additional Targeting Rules** - These are added to the experiment targeting rules. Useful if you want to limit to admin users or people who opt-in to beta experiences.

## Stopping

When you stop an experiment, you are prompted for a few things:

-  **Reason for stopping the test** - Provide context for why you are stopping. Did it reach significance?  Did you discover a bug?
-  **Conclusion** - Either `Did not finish`, `Inconclusive`, `Won`, or `Lost`.  If Won and you have more than 2 variations, it will prompt you for which one is the winner.
-  **Additional analysis or details** - If you did any analysis outside of the built-in reporting, explain that here. Sometimes an experiment "wins" on paper, but there are other factors at plan in the decision. Describe those here.

You are able to edit the Conclusion and Additional Analysis at any point after stopping the test, so don't worry if you aren't sure yet.

If you select Conclusion = "won", the client libraries will start sending 100% of traffic to the winning variation.
This is usually a temporary solution and you'll want to go back and clean up the implementation code at some point.

### Results

![Results Table](/images/results-table.png)

Each row of this table is a different metric.

**Chance to Beat Control** is a bayesian statistic telling you how likely it is that the variation is better.  Typically,
you want to wait until this reaches 95%.

**Percent Change** shows how much better/worse the variation is compared to the control.  The numbers show the median and the 95% confidence interval.
To generate these numbers, we use statistical bootstrapping and simulate your experiment 10,000 times.

If you have pre-defined dimensions for your users, you can use the **Dimension** dropdown to drill down into your results.
This is very useful for debugging (e.g. if Safari is down, but the other browser are fine, you may have an implementation bug).

Be careful, the more metrics and dimensions you look at, the more likely you are to see a false positive. If you find something that looks 
surprising, it's often worth a dedicated follow-up experiment to verify that it's real.