# Experiments

Experiments are the core of Growth Book.  This page covers several topics:

1.  Creating
2.  Targeting
3.  Implementation
4.  Starting and Stopping
5.  Results

## Creating

When you create a new experiment, it starts out as a draft and remains fully editable until you start it.

Experiment drafts are a great place to collaborate between PMs, designers, and engineers. 

PMs can spec out the requirements, designers can upload mockups and screenshots, and engineers can work on implementation.

Use the built-in discussion thread to add comments or collect feedback.

## Targeting

There are a few different ways to limit your experiment to a subset of users:

1.  Select specific user groups to include (e.g. internal employees, beta testers)
2.  Limit to logged-in users only
3.  Specify a URL regex pattern
4.  Custom targeting rules (e.g. `age > 18`)

All of these rules are evaluated locally in your app at runtime and no HTTP requests are made to the GrowthBook servers.

## Implementation

There are generally two ways to implement experiments. 
The first is using our [Visual Editor](/app/visual) and the second is using one of our [Client Libraries](/lib) (for Javascript, React, PHP, Ruby, or Python).

It's also possible to use a completely custom implementation (or another library like PlanOut).
The only requirement is that you track in your datasource when users are put into an experiment and which variation they received.

## Starting and Stopping

When you start an experiment, you will be prompted for how you want to split traffic between the variations and who you want to roll the test out to.  

When stopping an experiment, you'll be prompted to enter which variation (if any) won and why you are stopping the test.

### Client Library Integration

If you are using the Client Libraries to implement experiments, there are some additional steps you must take.  

The Client Libraries never communicate with the Growth Book servers.  That means as soon as your deploy the A/B test code to production, people will start getting put into the experiment immediately and the experiment will continue until you remove the code and do another deploy.

This separation has huge performance and reliability benefits (if Growth Book goes down, it has no effect on your app), but it can be a bit unintuitive when you press the "Stop" button in the UI and people continue to be put into the experiment.

To get the best of both worlds, you can store a cached copy of experiments in Redis (or similar) and keep it up-to-date either by periodically hitting the [Growth Book API](/api-docs) or setting up a [Webhook Endpoint](/app/webhooks). Then your app can query the cache at runtime to get the latest experiment statuses.
## Results

![Results Table](/images/results-table.png)

Each row of this table is a different metric.

**Chance to Beat Control** tells you the probability that the variation is better. The closer this gets to 100%, the more likely the variation is better.  The closer it gets to 0% the more likely the variation is worse.
Numbers in the middle are inconclusive and require more data.  We recommend waiting until it reaches 95% or 5% before deciding on a winner as a general rule of thumb.

**Risk** tells you how much you will lose if you choose the variation as the winner, but it ends up actually being worse (no matter how unlikely that is).

**Percent Change** shows how much better/worse the variation is compared to the control. It is a probability density graph and the thicker the area, the more likely the true percent change will be there.
As you collect more data, the tails of the graphs will shorten, indicating more certainty around the estimates.

### Dimensions

If you have defined dimensions for your users, you can use the **Dimension** dropdown to drill down into your results.
This is very useful for debugging (e.g. if Safari is down, but the other browser are fine, you may have an implementation bug).

Be careful, the more metrics and dimensions you look at, the more likely you are to see a false positive. If you find something that looks 
surprising, it's often worth a dedicated follow-up experiment to verify that it's real.