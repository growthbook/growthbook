# Experiments

Experiments are the core of Growth Book.  This page covers several topics:

1.  Creating
2.  Targeting
3.  Implementation
4.  Starting and Stopping
5.  Results

## Creating

When you create a new experiment, it starts out as a draft and remains fully editable until you start it.

Experiment drafts are a great place to collaborate between PMs, designers, and engineers. 

PMs can spec out the requirements, designers can upload mockups and screenshots, and engineers can work on implementation.

Use the built-in discussion thread to add comments or collect feedback.

## Targeting

There are a few different ways to limit your experiment to a subset of users:

1.  Specify a pre-defined **Segment**
2.  Limit to logged-in users only
3.  Specify a URL regex pattern
4.  Custom targeting rules (e.g. `age > 18`)

All of these rules are evaluated locally in your app at runtime and no HTTP requests are made to the GrowthBook servers.

## Implementation

We offer official client libraries in a few popular languages with more coming soon:

*  [Javascript/Typescript](/lib/js)
*  [React](/lib/react)
*  [PHP](/lib/php)
*  [Ruby](/lib/ruby)
*  Go - *coming soon*
*  Python - *coming soon*

It's not required to use these libraries.  You can do variation assignment yourself or use another library like PlanOut.

The only requirement is that you track in your datasource when users are put into an experiment and which variation they received.

## Starting and Stopping

The client libraries never communicate with the Growth Book servers.  So as soon as your deploy the A/B test code to production, people will start getting put into the experiment immediately.  And the experiment will continue until you remove the code and do another deploy.

If you want to be able to control experiments within the Growth Book App, you can integrate with the [API](/api-docs).

## Results

![Results Table](/images/results-table.png)

Each row of this table is a different metric.

**Chance to Beat Control** is a bayesian statistic telling you how likely it is that the variation is better.  Typically,
you want to wait until this reaches 95%.

**Percent Change** shows how much better/worse the variation is compared to the control.  The numbers show the median and the 95% confidence interval.
To generate these numbers, we use statistical bootstrapping and simulate your experiment 10,000 times.

If you have pre-defined dimensions for your users, you can use the **Dimension** dropdown to drill down into your results.
This is very useful for debugging (e.g. if Safari is down, but the other browser are fine, you may have an implementation bug).

Be careful, the more metrics and dimensions you look at, the more likely you are to see a false positive. If you find something that looks 
surprising, it's often worth a dedicated follow-up experiment to verify that it's real.