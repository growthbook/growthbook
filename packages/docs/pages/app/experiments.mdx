# Experiments

Experiments are the core of Growth Book.  This page covers several topics:

1.  Creating
2.  Targeting
3.  Implementation
4.  Starting and Stopping
5.  Results

## Creating

When you create a new experiment, it starts out as a draft and remains fully editable until you start it.

Experiment drafts are a great place to collaborate between PMs, designers, and engineers. 

PMs can spec out the requirements, designers can upload mockups and screenshots, and engineers can work on implementation.

Use the built-in discussion thread to add comments or collect feedback.

## Targeting

There are a few different ways to limit your experiment to a subset of users:

1.  Select specific user groups to include (e.g. internal employees, beta testers)
2.  Limit to logged-in users only
3.  Specify a URL regex pattern
4.  Custom targeting rules (e.g. `age > 18`)

All of these rules are evaluated locally in your app at runtime and no HTTP requests are made to the GrowthBook servers.

## Implementation

There are generally two ways to implement experiments. 
The first is using our [Visual Editor](/app/visual) and the second is using one of our [Client Libraries](/lib) (for Javascript, React, PHP, Ruby, or Python).

It's also possible to use a completely custom implementation (or another library like PlanOut).
The only requirement is that you track in your datasource when users are put into an experiment and which variation they received.

## Starting and Stopping

When you start an experiment, you will be prompted for how you want to split traffic between the variations and who you want to roll the test out to.  

When stopping an experiment, you'll be prompted to enter which variation (if any) won and why you are stopping the test.

### Client Library Integration

If you are using the Client Libraries to implement experiments, there are some additional steps you must take.  

The Client Libraries never communicate with the Growth Book servers.  That means as soon as your deploy the A/B test code to production, people will start getting put into the experiment immediately and the experiment will continue until you remove the code and do another deploy.

This separation has huge performance and reliability benefits (if Growth Book goes down, it has no effect on your app), but it can be a bit unintuitive when you press the "Stop" button in the UI and people continue to be put into the experiment.

To get the best of both worlds, you can integrate with the [API](/api-docs). In essense, you would periodically fetch and cache the latest experiment settings in something like Redis.  Then your app would query the cache to determine which experiments are running. You get the benefits of separation and the convenience of using the Growth Book UI to manage experiments.

## Results

![Results Table](/images/results-table.png)

Each row of this table is a different metric.

**Chance to Beat Control** tells you the probability that the variation is better. The closer this gets to 100%, the more likely the variation is better.  The closer it gets to 0% the more likely the variation is worse.
Numbers in the middle are inconclusive and require more data.  We recommend waiting until it reaches 95% or 5% before deciding on a winner as a general rule of thumb.

**Risk** quanitifies how risky a variation is.  If you choose the variation as a winner, but it ends up actually being worse (no matter how unlikely that is), the Risk will tell you how much you can expect to lose.
The number is always negative and the closer it is to zero, the less you risk by switching.

**Percent Change** shows how much better/worse the variation is compared to the control. It is a probability density graph and the thicker the area, the more likely the true percent change will be there.
As you collect more data, the tails of the graphs will shorten, indicating more certainty around the estimates.

### Dimensions

If you have defined dimensions for your users, you can use the **Dimension** dropdown to drill down into your results.
This is very useful for debugging (e.g. if Safari is down, but the other browser are fine, you may have an implementation bug).

Be careful, the more metrics and dimensions you look at, the more likely you are to see a false positive. If you find something that looks 
surprising, it's often worth a dedicated follow-up experiment to verify that it's real.