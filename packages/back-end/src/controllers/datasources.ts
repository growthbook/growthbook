import { Response } from "express";
import cloneDeep from "lodash/cloneDeep";
import * as bq from "@google-cloud/bigquery";
import { SQL_ROW_LIMIT } from "shared/sql";
import {
  PIPELINE_MODE_SUPPORTED_DATA_SOURCE_TYPES,
  getPipelineValidationCreateTableQuery,
  getPipelineValidationDropTableQuery,
  getPipelineValidationInsertQuery,
  type PipelineValidationResults,
} from "shared/enterprise";
import { AuthRequest } from "back-end/src/types/AuthRequest";
import { getContextFromReq } from "back-end/src/services/organizations";
import {
  DataSourceParams,
  DataSourceType,
  DataSourceSettings,
  DataSourceInterface,
  ExposureQuery,
  DataSourceInterfaceWithParams,
  GrowthbookClickhouseDataSource,
  MaterializedColumn,
  MaterializedColumnType,
  GrowthbookClickhouseSettings,
} from "back-end/types/datasource";
import {
  getSourceIntegrationObject,
  getNonSensitiveParams,
  mergeParams,
  encryptParams,
  testQuery,
  getIntegrationFromDatasourceId,
  runFreeFormQuery,
  runUserExposureQuery,
} from "back-end/src/services/datasource";
import { getOauth2Client } from "back-end/src/integrations/GoogleAnalytics";
import SqlIntegration from "back-end/src/integrations/SqlIntegration";
import {
  getQueriesByDatasource,
  getQueriesByIds,
} from "back-end/src/models/QueryModel";
import { findDimensionsByDataSource } from "back-end/src/models/DimensionModel";
import {
  createDataSource,
  getDataSourcesByOrganization,
  getDataSourceById,
  deleteDatasource,
  updateDataSource,
} from "back-end/src/models/DataSourceModel";
import { GoogleAnalyticsParams } from "back-end/types/integrations/googleanalytics";
import { getMetricsByDatasource } from "back-end/src/models/MetricModel";
import { deleteInformationSchemaById } from "back-end/src/models/InformationSchemaModel";
import { deleteInformationSchemaTablesByInformationSchemaId } from "back-end/src/models/InformationSchemaTablesModel";
import { queueCreateAutoGeneratedMetrics } from "back-end/src/jobs/createAutoGeneratedMetrics";
import { TemplateVariables } from "back-end/types/sql";
import { getUserById } from "back-end/src/models/UserModel";
import { AuditUserLoggedIn } from "back-end/types/audit";
import {
  createDimensionSlices,
  getDimensionSlicesById,
} from "back-end/src/models/DimensionSlicesModel";
import { DimensionSlicesQueryRunner } from "back-end/src/queryRunners/DimensionSlicesQueryRunner";
import {
  AutoMetricToCreate,
  SourceIntegrationInterface,
} from "back-end/src/types/Integration";
import { IS_CLOUD } from "back-end/src/util/secrets";
import {
  _dangerousRecreateClickhouseTables,
  createClickhouseUser,
  getReservedColumnNames,
  updateMaterializedColumns,
} from "back-end/src/services/clickhouse";
import { FactTableColumnType } from "back-end/types/fact-table";
import { factTableColumnTypes } from "back-end/src/routers/fact-table/fact-table.validators";
import { UNITS_TABLE_PREFIX } from "../queryRunners/ExperimentResultsQueryRunner";
import { getExperimentsByTrackingKeys } from "../models/ExperimentModel";

export async function deleteDataSource(
  req: AuthRequest<null, { id: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { org } = context;
  const { id } = req.params;

  const datasource = await getDataSourceById(context, id);
  if (!datasource) {
    throw new Error("Cannot find datasource");
  }

  if (!context.permissions.canDeleteDataSource(datasource)) {
    context.permissions.throwPermissionError();
  }

  // Make sure this data source isn't the organizations default
  if (org.settings?.defaultDataSource === datasource.id) {
    throw new Error(
      "Error: This is the default data source for your organization. You must select a new default data source in your Organization Settings before deleting this one.",
    );
  }

  // Make sure there are no metrics
  const metrics = await getMetricsByDatasource(context, datasource.id);
  if (metrics.length > 0) {
    throw new Error(
      "Error: Please delete all metrics tied to this datasource first.",
    );
  }

  // Make sure there are no segments
  const segments = await context.models.segments.getByDataSource(datasource.id);

  if (segments.length > 0) {
    throw new Error(
      "Error: Please delete all segments tied to this datasource first.",
    );
  }

  // Make sure there are no dimensions
  const dimensions = await findDimensionsByDataSource(
    datasource.id,
    datasource.organization,
  );
  if (dimensions.length > 0) {
    throw new Error(
      "Error: Please delete all dimensions tied to this datasource first.",
    );
  }

  await deleteDatasource(context, datasource);

  if (datasource.settings?.informationSchemaId) {
    const informationSchemaId = datasource.settings.informationSchemaId;

    await deleteInformationSchemaById(org.id, informationSchemaId);

    await deleteInformationSchemaTablesByInformationSchemaId(
      org.id,
      informationSchemaId,
    );
  }

  res.status(200).json({
    status: 200,
  });
}

export async function getDataSources(req: AuthRequest, res: Response) {
  const context = getContextFromReq(req);
  const datasources = await getDataSourcesByOrganization(context);

  if (!datasources || !datasources.length) {
    res.status(200).json({
      status: 200,
      datasources: [],
    });
    return;
  }

  res.status(200).json({
    status: 200,
    datasources: datasources.map((d) => {
      const integration = getSourceIntegrationObject(context, d);
      return {
        id: d.id,
        name: d.name,
        description: d.description,
        type: d.type,
        settings: d.settings,
        projects: d.projects ?? [],
        params: getNonSensitiveParams(integration),
      };
    }),
  });
}

export async function getDataSource(
  req: AuthRequest<null, { id: string }>,
  res: Response<DataSourceInterfaceWithParams>,
) {
  const context = getContextFromReq(req);
  const { id } = req.params;

  const integration = await getIntegrationFromDatasourceId(context, id);

  res.status(200).json(getDataSourceWithParams(integration));
}

function getDataSourceWithParams(
  integration: SourceIntegrationInterface,
): DataSourceInterfaceWithParams {
  const datasource = integration.datasource;

  // eslint-disable-next-line
  const { params, ...otherFields } = datasource;

  return {
    ...otherFields,
    params: getNonSensitiveParams(integration),
    decryptionError: integration.decryptionError,
  };
}

export async function postDataSources(
  req: AuthRequest<{
    name: string;
    description?: string;
    type: DataSourceType;
    params: DataSourceParams;
    settings: DataSourceSettings;
    projects?: string[];
  }>,
  res: Response<
    | {
        status: 200;
        id: string;
        datasource: DataSourceInterfaceWithParams;
      }
    | {
        status: 400;
        message: string;
      }
  >,
) {
  const context = getContextFromReq(req);
  const { name, description, type, params, projects } = req.body;
  const settings = req.body.settings || {};

  if (!context.permissions.canCreateDataSource({ projects, type })) {
    context.permissions.throwPermissionError();
  }

  try {
    // Set default event properties and queries
    settings.events = {
      experimentEvent: "$experiment_started",
      experimentIdProperty: "Experiment name",
      variationIdProperty: "Variant name",
      ...settings?.events,
    };

    const datasource = await createDataSource(
      context,
      name,
      type,
      params,
      settings,
      undefined,
      description,
      projects,
    );

    const integration = getSourceIntegrationObject(context, datasource);

    res.status(200).json({
      status: 200,
      id: datasource.id,
      datasource: getDataSourceWithParams(integration),
    });
  } catch (e) {
    res.status(400).json({
      status: 400,
      message: e.message || "An error occurred",
    });
  }
}

export async function postManagedWarehouse(
  req: AuthRequest,
  res: Response<
    | {
        status: 200;
        id: string;
        datasource: DataSourceInterfaceWithParams;
      }
    | {
        status: 400 | 403 | 404;
        message: string;
      }
  >,
) {
  if (!IS_CLOUD) {
    return res.status(403).json({
      status: 403,
      message: "This endpoint is only available in GrowthBook Cloud",
    });
  }

  const context = getContextFromReq(req);

  if (
    !context.superAdmin &&
    !context.permissions.canCreateDataSource({ type: "growthbook_clickhouse" })
  ) {
    context.permissions.throwPermissionError();
  }

  if (!context.superAdmin && !context.hasPremiumFeature("managed-warehouse")) {
    return res.status(403).json({
      status: 403,
      message: "This requires a Pro account.",
    });
  }

  // Start out with some default materialized columns
  // These can be changed by the user later
  const identifiers = ["device_id", "user_id"];
  const dimensions = [
    "geo_country",
    "ua_browser",
    "ua_os",
    "ua_device_type",
    "utm_source",
    "utm_medium",
    "utm_campaign",
  ];
  const materializedColumns: MaterializedColumn[] = [
    ...identifiers.map(
      (id) =>
        ({
          sourceField: id,
          columnName: id,
          datatype: "string",
          type: "identifier",
        }) as const,
    ),
    ...dimensions.map(
      (dim) =>
        ({
          sourceField: dim,
          columnName: dim,
          datatype: "string",
          type: "dimension",
        }) as const,
    ),
    {
      sourceField: "url_path",
      columnName: "url_path",
      datatype: "string",
      type: "",
    },
  ];

  const params = await createClickhouseUser(context, materializedColumns);
  const datasourceSettings = getManagedWarehouseSettings(
    materializedColumns,
    {},
  );

  const datasource = await createDataSource(
    context,
    "Managed Warehouse",
    "growthbook_clickhouse",
    params,
    datasourceSettings,
    "managed_warehouse",
  );

  const integration = getSourceIntegrationObject(context, datasource);

  res.status(200).json({
    status: 200,
    id: "managed_warehouse",
    datasource: getDataSourceWithParams(integration),
  });
}

export async function putDataSource(
  req: AuthRequest<
    {
      name?: string;
      description?: string;
      type?: DataSourceType;
      params?: DataSourceParams;
      settings?: DataSourceSettings;
      projects?: string[];
      metricsToCreate?: AutoMetricToCreate[];
    },
    { id: string }
  >,
  res: Response<
    | {
        status: 200;
        datasource: DataSourceInterfaceWithParams;
      }
    | {
        status: 400 | 403 | 404;
        message: string;
      }
  >,
) {
  const userId = req.userId;

  if (!userId) {
    res.status(403).json({
      status: 403,
      message: "User not found",
    });
    return;
  }

  const user = await getUserById(userId);

  if (!user) {
    res.status(403).json({
      status: 403,
      message: "User not found",
    });
    return;
  }

  const userObj: AuditUserLoggedIn = {
    id: user.id,
    email: user.email,
    name: user.name || "",
  };
  const context = getContextFromReq(req);
  const { org } = context;
  const { id } = req.params;
  const {
    name,
    description,
    type,
    params,
    settings,
    projects,
    metricsToCreate,
  } = req.body;

  const datasource = await getDataSourceById(context, id);
  if (!datasource) {
    res.status(404).json({
      status: 404,
      message: "Cannot find data source",
    });
    return;
  }

  if (!context.permissions.canUpdateDataSourceSettings(datasource)) {
    context.permissions.throwPermissionError();
  }

  // Require higher permissions to change connection settings vs updating query settings
  if (params) {
    if (!context.permissions.canUpdateDataSourceParams(datasource)) {
      context.permissions.throwPermissionError();
    }
  }

  // If changing projects, make sure the user has access to the new projects as well
  if (projects) {
    if (!context.permissions.canUpdateDataSourceSettings({ projects })) {
      context.permissions.throwPermissionError();
    }
  }

  if (type && type !== datasource.type) {
    res.status(400).json({
      status: 400,
      message:
        "Cannot change the type of an existing data source. Create a new one instead.",
    });
    return;
  }

  if (metricsToCreate?.length) {
    await queueCreateAutoGeneratedMetrics(
      datasource.id,
      org.id,
      metricsToCreate,
      userObj,
    );
  }

  try {
    const updates: Partial<DataSourceInterface> = { dateUpdated: new Date() };

    if (name) {
      updates.name = name;
    }

    if ("description" in req.body) {
      updates.description = description;
    }

    if (settings) {
      updates.settings = settings;
    }

    if (projects) {
      updates.projects = projects;
    }

    if (
      type === "google_analytics" &&
      params &&
      (params as GoogleAnalyticsParams).refreshToken
    ) {
      const oauth2Client = getOauth2Client();
      const { tokens } = await oauth2Client.getToken(
        (params as GoogleAnalyticsParams).refreshToken,
      );
      (params as GoogleAnalyticsParams).refreshToken =
        tokens.refresh_token || "";
    }

    // If the connection params changed, re-validate the connection
    // If the user is just updating the display name, no need to do this
    if (params) {
      const integration = getSourceIntegrationObject(context, datasource);
      mergeParams(integration, params);
      await integration.testConnection();
      updates.params = encryptParams(integration.params);
    }

    await updateDataSource(context, datasource, updates);

    const integration = getSourceIntegrationObject(context, {
      ...datasource,
      ...updates,
    });

    res.status(200).json({
      status: 200,
      datasource: getDataSourceWithParams(integration),
    });
  } catch (e) {
    req.log.error(e, "Failed to update data source");
    res.status(400).json({
      status: 400,
      message: e.message || "An error occurred",
    });
  }
}

/**
 * Validates the pipeline settings for the given data source.
 * Ensures we can create, insert and drop tables.
 */
export async function postValidatePipelineSettings(
  req: AuthRequest<
    {
      pipelineSettings: DataSourceSettings["pipelineSettings"];
    },
    { id: string }
  >,
  res: Response<
    | { message: string }
    | { tableName?: string; results: PipelineValidationResults }
  >,
) {
  const { id } = req.params;
  const context = getContextFromReq(req);
  const datasource = await getDataSourceById(context, id);
  if (!datasource) {
    return res.status(404).json({ message: "Cannot find data source" });
  }

  if (!context.permissions.canRunPipelineValidationQueries(datasource)) {
    context.permissions.throwPermissionError();
  }

  const { pipelineSettings } = req.body;
  if (!pipelineSettings) {
    return res.status(400).json({
      message: "Pipeline settings are required to be validated",
    });
  }

  if (
    !PIPELINE_MODE_SUPPORTED_DATA_SOURCE_TYPES[pipelineSettings.mode].includes(
      datasource.type,
    )
  ) {
    return res.status(400).json({
      message: `This data source does not support pipeline mode ${pipelineSettings.mode}`,
    });
  }

  if (!pipelineSettings.allowWriting) {
    return res.status(200).json({
      results: {
        create: {
          result: "skipped",
          resultMessage: "Skipped because allowWriting is false",
        },
      },
    });
  }

  const integration = getSourceIntegrationObject(context, {
    ...datasource,
    settings: {
      ...datasource.settings,
      pipelineSettings: {
        ...pipelineSettings,
        // Ensure minimum retention hours for testing
        unitsTableRetentionHours: 1,
      },
    },
  });

  if (!(integration instanceof SqlIntegration)) {
    return res.status(400).json({
      message: "This data source does not support ad-hoc validation.",
    });
  }

  const randomSuffix = Math.random().toString(36).substring(2, 7);
  const testTableName = `${UNITS_TABLE_PREFIX}_validation_${randomSuffix}`;

  const fullTestTablePath = integration.generateTablePath(
    testTableName,
    pipelineSettings.writeDataset,
    pipelineSettings.writeDatabase,
    true,
  );

  const results: PipelineValidationResults = {
    create: { result: "skipped" },
  };

  try {
    await integration.runTestQuery(
      await getPipelineValidationCreateTableQuery({
        tableFullName: fullTestTablePath,
        integration,
      }),
    );
    results.create.result = "success";
  } catch (e) {
    results.create = {
      result: "failed",
      resultMessage: "message" in e ? e.message : String(e),
    };
  }

  // Insert only happens for incremental mode
  if (pipelineSettings.mode === "incremental") {
    if (results.create.result !== "success") {
      results.insert = {
        result: "skipped",
        resultMessage: "Skipped due to create failure",
      };
    } else {
      try {
        await integration.runTestQuery(
          getPipelineValidationInsertQuery({
            tableFullName: fullTestTablePath,
            integration,
          }),
        );
        results.insert = { result: "success" };
      } catch (e) {
        results.insert = {
          result: "failed",
          resultMessage: "message" in e ? e.message : String(e),
        };
      }
    }
  }

  if (
    pipelineSettings.mode === "incremental" ||
    (pipelineSettings.mode === "ephemeral" &&
      integration.dropUnitsTable() &&
      pipelineSettings.unitsTableDeletion === true)
  ) {
    if (results.create.result !== "success") {
      results.drop = {
        result: "skipped",
        resultMessage: "Skipped due to create failure",
      };
    } else {
      try {
        await integration.runTestQuery(
          getPipelineValidationDropTableQuery({
            tableFullName: fullTestTablePath,
            integration,
          }),
        );
        results.drop = { result: "success" };
      } catch (e) {
        results.drop = {
          result: "failed",
          resultMessage: "message" in e ? e.message : String(e),
        };
      }
    }
  }

  res.status(200).json({ tableName: fullTestTablePath, results });
}

export async function updateExposureQuery(
  req: AuthRequest<
    {
      updates: Partial<ExposureQuery>;
    },
    { datasourceId: string; exposureQueryId: string }
  >,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { datasourceId, exposureQueryId } = req.params;
  const { updates } = req.body;

  const dataSource = await getDataSourceById(context, datasourceId);
  if (!dataSource) {
    res.status(404).json({
      status: 404,
      message: "Cannot find data source",
    });
    return;
  }

  if (!context.permissions.canUpdateDataSourceSettings(dataSource)) {
    context.permissions.throwPermissionError();
  }

  const copy = cloneDeep<DataSourceInterface>(dataSource);
  const exposureQueryIndex = copy.settings.queries?.exposure?.findIndex(
    (e) => e.id === exposureQueryId,
  );
  if (
    exposureQueryIndex === undefined ||
    !copy.settings.queries?.exposure?.[exposureQueryIndex]
  ) {
    res.status(404).json({
      status: 404,
      message: "Cannot find exposure query",
    });
    return;
  }

  const exposureQuery = copy.settings.queries.exposure[exposureQueryIndex];
  copy.settings.queries.exposure[exposureQueryIndex] = {
    ...exposureQuery,
    ...updates,
  };

  try {
    const updates: Partial<DataSourceInterface> = {
      dateUpdated: new Date(),
      settings: copy.settings,
    };

    await updateDataSource(context, dataSource, updates);

    res.status(200).json({
      status: 200,
    });
  } catch (e) {
    req.log.error(e, "Failed to update exposure query");
    res.status(400).json({
      status: 400,
      message: e.message || "An error occurred",
    });
  }
}

export async function postGoogleOauthRedirect(
  req: AuthRequest<{ projects?: string[] }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { projects } = req.body;

  if (
    !context.permissions.canCreateDataSource({
      projects,
      type: "google_analytics",
    })
  ) {
    context.permissions.throwPermissionError();
  }

  const oauth2Client = getOauth2Client();

  const url = oauth2Client.generateAuthUrl({
    // eslint-disable-next-line
    access_type: "offline",
    // eslint-disable-next-line
    include_granted_scopes: true,
    prompt: "consent",
    scope: "https://www.googleapis.com/auth/analytics.readonly",
  });

  res.status(200).json({
    status: 200,
    url,
  });
}

export async function getQueries(
  req: AuthRequest<null, { ids: string }>,
  res: Response,
) {
  const { org } = getContextFromReq(req);
  const { ids } = req.params;
  const queries = ids.split(",");

  const docs = await getQueriesByIds(org.id, queries);

  // Lookup table so we can return queries in the same order we received them
  const map = new Map(docs.map((d) => [d.id, d]));

  res.status(200).json({
    queries: queries.map((id) => map.get(id) || null),
  });
}

export async function testLimitedQuery(
  req: AuthRequest<{
    query: string;
    datasourceId: string;
    templateVariables?: TemplateVariables;
    limit?: number;
  }>,
  res: Response,
) {
  const context = getContextFromReq(req);

  const { query, datasourceId, templateVariables, limit } = req.body;

  // Sanity check to prevent potential abuse
  if (limit && limit > SQL_ROW_LIMIT) {
    return res.status(400).json({
      status: 400,
      message: `Limit clause cannot be greater than ${SQL_ROW_LIMIT}`,
    });
  }

  const maxLimit = limit || SQL_ROW_LIMIT;

  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    return res.status(404).json({
      status: 404,
      message: "Cannot find data source",
    });
  }

  const { results, sql, duration, error } = await testQuery(
    context,
    datasource,
    query,
    templateVariables,
    maxLimit,
  );

  res.status(200).json({
    status: 200,
    duration,
    results,
    sql,
    error,
  });
}

export async function runQuery(
  req: AuthRequest<{
    query: string;
    datasourceId: string;
    limit?: number;
  }>,
  res: Response,
) {
  const context = getContextFromReq(req);

  const { query, datasourceId, limit } = req.body;

  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    return res.status(404).json({
      status: 404,
      message: "Cannot find data source",
    });
  }

  const { results, sql, duration, error } = await runFreeFormQuery(
    context,
    datasource,
    query,
    limit,
  );

  res.status(200).json({
    status: 200,
    duration,
    results,
    sql,
    error,
  });
}

export async function runUserExperimentExposuresQuery(
  req: AuthRequest<{
    unitId: string;
    userIdType: string;
    lookbackDays: number;
    datasourceId: string;
  }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { unitId, userIdType, lookbackDays, datasourceId } = req.body;
  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    res.status(404).json({
      status: 404,
      message: "Cannot find data source",
    });
    return;
  }

  const { rows, statistics, error, sql } = await runUserExposureQuery(
    context,
    datasource,
    unitId,
    userIdType,
    lookbackDays,
  );

  // If there are rows, construct a map of experiment_id (which is really the experiment tracking key) to the experiment id
  let experimentMap = new Map<string, string>();
  if (rows) {
    const experimentTrackingKeys = rows.map((row) => row.experiment_id);
    const uniqueExperimentTrackingKeys = new Set(experimentTrackingKeys);
    const experiments = await getExperimentsByTrackingKeys(
      context,
      Array.from(uniqueExperimentTrackingKeys),
    );
    experimentMap = new Map(
      experiments.map((experiment) => [experiment.trackingKey, experiment.id]),
    );
  }

  res.status(200).json({
    status: 200,
    rows,
    experimentMap: Object.fromEntries(experimentMap),
    statistics,
    error,
    sql,
  });
}

export async function getDataSourceMetrics(
  req: AuthRequest<null, { id: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { id } = req.params;

  const metrics = await getMetricsByDatasource(context, id);

  res.status(200).json({
    status: 200,
    metrics,
  });
}

export async function getDataSourceQueries(
  req: AuthRequest<null, { id: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { id } = req.params;

  const datasourceObj = await getDataSourceById(context, id);
  if (!datasourceObj) {
    throw new Error("Could not find datasource");
  }

  req.checkPermissions(
    "readData",
    datasourceObj?.projects?.length ? datasourceObj.projects : [],
  );

  const queries = await getQueriesByDatasource(context.org.id, id);

  res.status(200).json({
    status: 200,
    queries,
  });
}

export async function getDimensionSlices(
  req: AuthRequest<null, { id: string }>,
  res: Response,
) {
  const { org } = getContextFromReq(req);
  const { id } = req.params;

  const dimensionSlices = await getDimensionSlicesById(org.id, id);

  res.status(200).json({
    status: 200,
    dimensionSlices,
  });
}

export async function postDimensionSlices(
  req: AuthRequest<{
    dataSourceId: string;
    queryId: string;
    lookbackDays: number;
  }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { org } = context;
  const { dataSourceId, queryId, lookbackDays } = req.body;

  const integration = await getIntegrationFromDatasourceId(
    context,
    dataSourceId,
    true,
  );

  const model = await createDimensionSlices({
    organization: org.id,
    dataSourceId,
    queryId,
  });

  const queryRunner = new DimensionSlicesQueryRunner(
    context,
    model,
    integration,
  );
  const outputmodel = await queryRunner.startAnalysis({
    exposureQueryId: queryId,
    lookbackDays: Number(lookbackDays) ?? 30,
  });
  res.status(200).json({
    status: 200,
    dimensionSlices: outputmodel,
  });
}

export async function cancelDimensionSlices(
  req: AuthRequest<null, { id: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { org } = context;
  const { id } = req.params;
  const dimensionSlices = await getDimensionSlicesById(org.id, id);
  if (!dimensionSlices) {
    throw new Error("Could not cancel automatic dimension");
  }

  const integration = await getIntegrationFromDatasourceId(
    context,
    dimensionSlices.datasource,
    true,
  );

  const queryRunner = new DimensionSlicesQueryRunner(
    context,
    dimensionSlices,
    integration,
  );
  await queryRunner.cancelQueries();

  res.status(200).json({
    status: 200,
  });
}

export async function fetchBigQueryDatasets(
  req: AuthRequest<{
    projectId: string;
    client_email: string;
    private_key: string;
  }>,
  res: Response,
) {
  const { projectId, client_email, private_key } = req.body;

  try {
    const client = new bq.BigQuery({
      projectId,
      credentials: { client_email, private_key },
    });

    const [datasets] = await client.getDatasets();

    res.status(200).json({
      status: 200,
      datasets: datasets.map((dataset) => dataset.id).filter(Boolean),
    });
  } catch (e) {
    throw new Error(e.message);
  }
}

export async function postMaterializedColumn(
  req: AuthRequest<
    {
      sourceField: string;
      columnName: string;
      datatype: FactTableColumnType;
      type?: MaterializedColumnType;
    },
    { datasourceId: string }
  >,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { datasourceId } = req.params;
  const newColumn = sanitizeMatColumnInput(req.body);

  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    throw new Error("Cannot find datasource");
  }

  if (!context.permissions.canUpdateDataSourceSettings(datasource)) {
    context.permissions.throwPermissionError();
  }

  if (datasource.type !== "growthbook_clickhouse") {
    throw new Error(
      "Can only create materialized columns for growthbook-clickhouse datasources",
    );
  }

  const originalColumns = datasource.settings.materializedColumns || [];
  const finalColumns = [...originalColumns, newColumn];

  // Make sure it doesn't exist already
  if (
    originalColumns.some(
      (col) =>
        col.columnName === newColumn.columnName ||
        col.sourceField === newColumn.sourceField,
    )
  ) {
    throw new Error(`That materialized column already exists`);
  }

  const updates = {
    settings: getManagedWarehouseSettings(finalColumns, datasource.settings),
  };

  try {
    await updateMaterializedColumns({
      context,
      datasource,
      columnsToAdd: [newColumn],
      columnsToDelete: [],
      columnsToRename: [],
      finalColumns,
      originalColumns,
    });

    await updateDataSource(context, datasource, updates);

    const integration = getSourceIntegrationObject(context, {
      ...datasource,
      ...updates,
    });

    res.status(200).json({
      status: 200,
      datasource: getDataSourceWithParams(integration),
    });
  } catch (e) {
    req.log.error(e, "Failed to update data source");
    res.status(500).json({
      status: 500,
      message: e.message || "An error occurred",
    });
  }
}

export async function updateMaterializedColumn(
  req: AuthRequest<
    {
      sourceField: string;
      columnName: string;
      datatype: FactTableColumnType;
      type?: MaterializedColumnType;
    },
    { datasourceId: string; matColumnName: string }
  >,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { datasourceId, matColumnName } = req.params;
  const newColumn = sanitizeMatColumnInput(req.body);

  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    throw new Error("Cannot find datasource");
  }

  if (!context.permissions.canUpdateDataSourceSettings(datasource)) {
    context.permissions.throwPermissionError();
  }

  if (datasource.type !== "growthbook_clickhouse") {
    throw new Error(
      "Can only manage materialized columns for growthbook-clickhouse datasources",
    );
  }

  const originalColumns = datasource.settings.materializedColumns || [];

  const originalIdx = originalColumns.findIndex(
    (col) => col.columnName === matColumnName,
  );
  if (originalIdx === -1) {
    throw new Error(`Cannot find materialized column ${matColumnName}`);
  }
  const originalColumn = originalColumns[originalIdx];

  const requiresDropAdd =
    originalColumn.datatype !== newColumn.datatype ||
    originalColumn.sourceField !== newColumn.sourceField;

  // When doing drop/add the column gets added to the end of the list
  // Otherwise it gets replaced in the same position
  const finalColumns = requiresDropAdd
    ? [
        ...originalColumns.slice(0, originalIdx),
        ...originalColumns.slice(originalIdx + 1),
        newColumn,
      ]
    : [
        ...originalColumns.slice(0, originalIdx),
        newColumn,
        ...originalColumns.slice(originalIdx + 1),
      ];

  // Make sure there is only 1 column with this source field or name
  if (
    finalColumns.filter(
      (col) =>
        col.sourceField === newColumn.sourceField ||
        col.columnName === newColumn.columnName,
    ).length > 1
  ) {
    throw new Error(`A materialized column for that attribute already exists`);
  }

  const updates = {
    settings: getManagedWarehouseSettings(finalColumns, datasource.settings),
  };

  try {
    if (requiresDropAdd) {
      if (matColumnName === newColumn.columnName) {
        throw new Error(
          "Cannot modify a column while keeping the same name. Delete the column and create it again instead",
        );
      }

      // Drop original column and recreate
      await updateMaterializedColumns({
        context,
        datasource,
        columnsToAdd: [newColumn],
        columnsToDelete: [originalColumn.columnName],
        columnsToRename: [],
        finalColumns,
        originalColumns,
      });
    } else if (originalColumn.columnName !== newColumn.columnName) {
      // Rename column
      await updateMaterializedColumns({
        context,
        datasource,
        columnsToAdd: [],
        columnsToDelete: [],
        columnsToRename: [
          { from: originalColumn.columnName, to: newColumn.columnName },
        ],
        finalColumns,
        originalColumns,
      });
    }

    await updateDataSource(context, datasource, updates);

    const integration = getSourceIntegrationObject(context, {
      ...datasource,
      ...updates,
    });

    res.status(200).json({
      status: 200,
      datasource: getDataSourceWithParams(integration),
    });
  } catch (e) {
    req.log.error(e, "Failed to update data source");
    res.status(500).json({
      status: 500,
      message: e.message || "An error occurred",
    });
  }
}

export async function deleteMaterializedColumn(
  req: AuthRequest<null, { datasourceId: string; matColumnName: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);
  const { datasourceId, matColumnName } = req.params;

  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    throw new Error("Cannot find datasource");
  }

  if (!context.permissions.canUpdateDataSourceSettings(datasource)) {
    context.permissions.throwPermissionError();
  }

  if (datasource.type !== "growthbook_clickhouse") {
    throw new Error(
      "Can only manage materialized columns for growthbook-clickhouse datasources",
    );
  }

  const originalColumns = datasource.settings.materializedColumns || [];

  const originalIdx = originalColumns.findIndex(
    (col) => col.columnName === matColumnName,
  );
  if (originalIdx === -1) {
    throw new Error(`Cannot find materialized column ${matColumnName}`);
  }
  const finalColumns = [
    ...originalColumns.slice(0, originalIdx),
    ...originalColumns.slice(originalIdx + 1),
  ];

  const updates = {
    settings: getManagedWarehouseSettings(finalColumns, datasource.settings),
  };

  try {
    await updateMaterializedColumns({
      context,
      datasource,
      columnsToAdd: [],
      columnsToDelete: [matColumnName],
      columnsToRename: [],
      finalColumns,
      originalColumns,
    });
    await updateDataSource(context, datasource, updates);

    const integration = getSourceIntegrationObject(context, {
      ...datasource,
      ...updates,
    });

    res.status(200).json({
      status: 200,
      datasource: getDataSourceWithParams(integration),
    });
  } catch (e) {
    req.log.error(e, "Failed to update data source");
    res.status(500).json({
      status: 500,
      message: e.message || "An error occurred",
    });
  }
}

export async function postRecreateManagedWarehouse(
  req: AuthRequest<null, { datasourceId: string }>,
  res: Response,
) {
  const context = getContextFromReq(req);

  // Escape hatch for super admins to re-generate the database and backfill data
  if (!context.superAdmin) {
    throw new Error(
      "You must be a super admin to recreate a Managed Warehouse datasource",
    );
  }

  const { datasourceId } = req.params;
  const datasource = await getDataSourceById(context, datasourceId);
  if (!datasource) {
    throw new Error("Cannot find datasource");
  }

  if (datasource.type !== "growthbook_clickhouse") {
    throw new Error(
      "Can only recreate a Managed Warehouse datasource, not other types",
    );
  }

  await _dangerousRecreateClickhouseTables(context, datasource);

  res.status(200).json({
    status: 200,
  });
}

function sanitizeMatColumnInput(
  userInput: MaterializedColumn,
): MaterializedColumn {
  if (!factTableColumnTypes.includes(userInput.datatype)) {
    throw new Error("Invalid datatype");
  }
  const sourceField = sanitizeMatColumnSourceField(userInput.sourceField);
  const columnName = sanitizeMatColumnName(userInput.columnName);

  let type = userInput.type;
  if (type === "dimension") {
    if (!["string", "number", "boolean"].includes(userInput.datatype)) {
      type = "";
    }
  } else if (type === "identifier") {
    if (!["string", "number"].includes(userInput.datatype)) {
      type = "";
    }
  } else {
    // In case some other unexpected string was passed in
    type = "";
  }

  return {
    datatype: userInput.datatype,
    sourceField,
    columnName,
    type,
  };
}

function sanitizeMatColumnSourceField(userInput: string) {
  // Invalid characters
  if (!/^[a-zA-Z0-9 _-]*$/.test(userInput)) {
    throw new Error(
      "Invalid input. Source field must only use alphanumeric characters, ' ', '_', or '-'",
    );
  }
  // Must have at least 1 alpha character
  if (!/[a-zA-Z]/.test(userInput)) {
    throw new Error(
      "Invalid input. Source field must contain at least one letter",
    );
  }
  // Must not have leading or trailing spaces
  if (userInput.startsWith(" ") || userInput.endsWith(" ")) {
    throw new Error(
      "Invalid input. Source field must not have leading or trailing spaces",
    );
  }

  return userInput;
}

function sanitizeMatColumnName(userInput: string) {
  if (!/^[a-zA-Z_][a-zA-Z0-9_]*$/.test(userInput)) {
    throw new Error(
      "Invalid input. Column names must start with a letter or underscore and only use alphanumeric characters or '_'",
    );
  }

  const cmp = userInput.toLowerCase();

  // Make sure the columns don't overwrite default ones we define
  const reservedCols = getReservedColumnNames();
  if (reservedCols.has(cmp)) {
    throw new Error(
      `Column name "${userInput}" is reserved and cannot be used`,
    );
  }

  // Most of these technically work as column names in ClickHouse,
  // but they would be confusing when writing and viewing SQL
  const sqlKeywords = new Set([
    "select",
    "from",
    "where",
    "order",
    "having",
    "limit",
    "offset",
    "join",
    "on",
    "using",
    "as",
    "distinct",
    "union",
    "if",
    "then",
    "else",
    "end",
    "case",
    "when",
    "and",
    "or",
    "not",
    "true",
    "false",
    "null",
    "is",
    "in",
    "between",
    "exists",
    "like",
    "array",
    "tuple",
    "map",
    "cast",
    "inf",
    "infinity",
    "nan",
    "default",
    "current_date",
    "current_timestamp",
    "sysdate",
  ]);
  if (sqlKeywords.has(cmp)) {
    throw new Error(
      `Column name "${userInput}" is a SQL keyword and cannot be used`,
    );
  }

  return userInput;
}

function getManagedWarehouseSettings(
  materializedColumns: MaterializedColumn[],
  existing: GrowthbookClickhouseDataSource["settings"],
): GrowthbookClickhouseSettings {
  // Require at least 1 identifier type
  if (!materializedColumns.some((c) => c.type === "identifier")) {
    throw new Error("Must have at least 1 identifier");
  }

  return {
    ...existing,
    materializedColumns,
    userIdTypes: materializedColumns
      .filter((c) => c.type === "identifier")
      .map((c) => ({
        userIdType: c.columnName,
        description: "",
      })),
    queries: {
      ...existing.queries,
      exposure: generateManagedWarehouseExposureQueries(materializedColumns),
    },
  };
}

function generateManagedWarehouseExposureQueries(
  materializedColumns: MaterializedColumn[],
): ExposureQuery[] {
  const identifiers = materializedColumns
    .filter((c) => c.type === "identifier")
    .map((c) => c.columnName);

  const dimensions = materializedColumns
    .filter((c) => c.type === "dimension")
    .map((c) => c.columnName);

  return identifiers.map((identifier) => {
    const cols = [
      identifier,
      "timestamp",
      "experiment_id",
      "variation_id",
      ...dimensions,
    ];

    return {
      id: identifier,
      dimensions,
      name: identifier,
      userIdType: identifier,
      query: `
SELECT 
  ${cols.join(",\n  ")}
FROM experiment_views
WHERE
  experiment_id LIKE '{{ experimentId }}'
  AND timestamp BETWEEN '{{startDate}}' AND '{{endDate}}'`.trim(),
    };
  });
}
