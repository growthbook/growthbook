---
title: Data Pipeline
description: Learn about enabling Pipeline Mode and improving query efficiency
sidebar_label: Data Pipeline
slug: /app/data-pipeline
---

# Data Pipeline Mode

:::info Enterprise only

Pipeline mode is only available for Enterprise customers and is currently only available for BigQuery, Snowflake, and Databricks Data Sources.

:::

For experimenters who have multiple metrics per experiment and have large experiment assignment sources, GrowthBook can greatly improve the performance of your queries if you enable **Pipeline Mode**, writing intermediate tables back to your warehouse and re-using those across metric analyses in an experiment. This depends a bit on your datasource cost structure, but if you are billed by rows scanned, pipeline mode will almost certainly provide substantial savings.

Pipeline mode supports two strategies:

- **Incremental refresh (Recommended)** — On each update, only scans new rows in your experiment assignment and metric sources, and persists reusable intermediate tables across updates to minimize re-scans and speed up analyses.
- **Ephemeral (Legacy)** — Per-analysis, short-lived tables are created with limited retention for reuse within an analysis window.

Both strategies do not access any additional data beyond what a non-pipeline run would access.

::::note

Availability may differ by warehouse and strategy. If you don't see an option in your datasource page within GrowthBook, please contact support.

::::

## Incremental refresh (Recommended)

Our main pipeline mode reduces cost and speeds up experiment updates by scanning only new data since the last run, while keeping a small set of reusable, de-duplicated tables in your warehouse.

### How it works

On each experiment update, GrowthBook:

1. Scans the experiment assignment source for any new rows that have a timestamp greater than the max timestamp seen on the previous run and updates a de-duplicated units table (one row per unit with timestamps and experiment dimensions).
2. Scans each metric's fact source for any new rows that have a timestamp greater than the max timestamp seen on the previous run and updates pre-aggregated metric tables (one row per unit-date-refresh with timestamps and metric dimensions).
3. Optionally loads pre-experiment windows for CUPED into a separate table for only newly seen units.
4. Joins the above to produce summary statistics and per-dimension breakdowns.

Therefore, GrowthBook will materialize the following tables per experiment:

- The units table (`Experiment Units Table` - named `gb_units_<experiment_id>` in your warehouse)
  - one row per unit in the experiment with the unit id, variation id, first exposure timestamp, and experiment dimensions
  - this table is joined to new exposures, de-duped, and then re-created on each update
  - this table is partitioned on ...
- One or more metrics tables depending on the number of metric sources and the number of metrics (named `gb_metrics_<experiment_id>_<metric_source_id>_<random_id>` in your warehouse)
  - one row per unit-date-refresh, as we take new metric values and aggregate them by unit and date on each refresh and insert them into the metrics table
  - this table is incrementally inserted into on each update
  - this table is partitioned on ...
- An additional table per above metric source for CUPED (named `gb_cuped_<experiment_id>_<metric_source_id>_<random_id>_covariate` in your warehouse)
  - one row per unit, as we scan the metric source for the pre-experiment window, join it to only new units found in the units update, and then insert the aggregated values per unit into the CUPED table
  - this table is incrementally inserted into on each update

Each update only scans data later max timestamp found within the last data scan. This means that if any new data lands in the data sources between experiment updates in GrowthBook, it must have a later timestamp than the data as of the last update or it will not be included in the analysis. This also means that if a Fact Table or Experiment Assignment Table definition includes references to multiple source tables, these tables should be updated at the same time to ensure GrowthBook can capture all data.

GrowthBook is building functionality to support additional use cases, improve control over intermediate table retention, and support additional data sources. Please reach out to support if you have any concerns or questions.

### Requirements

- A dedicated schema/dataset with write access for GrowthBook
- Ability to retain intermediate tables beyond 24 hours (often days) for reuse
- Appropriate permissions to create/read/update (and when supported, merge/partition) tables
- Experiment assignment queries and fact tables partitioned on the `timestamp` column

### Controls and observability

- Default "Update" performs an incremental refresh
- "Full refresh" is available when you need to re-state results (e.g., backfills, config changes)
- View the queries GrowthBook runs and the intermediate table names, with last updated timestamps, in the View Queries modal on an experiment page.

### Limitations and notes

- While in Beta, some analysis tools (activation metrics, quantile metrics, ratio metrics with different numerator and denominator fact tables) are not supported
- Changes to analysis settings sometimes require a full refresh of all of the data to take effect. In the future, we will make improvements to ensure that only the data that need be restated is restated.
- Exact table schemas and partitioning/retention strategies vary by warehouse.

### Enable Incremental refresh

1. In your Data Source, open "Data Pipeline Settings"
2. Enable Pipeline Mode and select the strategy: "Incremental refresh (Recommended)"
3. Set a destination schema/dataset for intermediate tables and choose retention settings
4. Save

You can test incremental refresh by enabling it only for a single experiment in the Data Pipeline Settings UI. We do not apply it to old experiments in the case that they have incompatible settings. To confirm pipeline mode is being used, you can look at the queries run for your experiment and validate that they are creating intermediate tables.

## Configuring Pipeline Mode

:::info Pipeline mode requires write permissions

While Pipeline mode requires you to give write permissions to your Data Source, GrowthBook only uses it for the Pipeline Mode. All other user-provided SQL statements are validated to be read-only before being executed.

:::

### BigQuery

1. (strongly recommended, but optional) Create a dedicated dataset to which GrowthBook will write tables. This will keep your data warehouse clean and ensure that we are only writing to a dedicated space.
2. Grant permissions to create tables to the role connecting GrowthBook to your warehouse. You can do this by granting your GrowthBook Service Account the `BigQuery Data Editor` role on the new dataset. You can also give only BigQuery table reading and writing permissions on that dataset if you want to be more restrictive.
3. Navigate to your BigQuery Data Source in GrowthBook and scroll down to "Data Pipeline Settings"
4. Click "Edit" and enable pipeline mode, set the destination dataset to your new dedicated GrowthBook dataset from step 1.

### Snowflake

1. (strongly recommended, but optional) Create a dedicated schema to which GrowthBook will write temporary tables. This will keep your data warehouse clean and ensure that we are only writing to a dedicated space.
2. Grant permissions to create tables to the role connecting GrowthBook to your warehouse. The Snowflake role attached to GrowthBook will need `CREATE TABLE`, `SELECT - FUTURE TABLE`, and `USAGE` on the schema created in step 1.
3. Navigate to your Snowflake Data Source in GrowthBook and scroll down to "Data Pipeline Settings"
4. Click "Edit" and enable pipeline mode, set the destination schema to your new dedicated GrowthBook schema from step 1, and set the number of hours you will retain our temporary tables. For Snowflake, we recommend leaving the value at 24 as Snowflake's retention is set in days and we will round up to the nearest day.

### Databricks

Databricks works slightly differently. Instead of creating a temporary table, we create a regular table for the deduped units assignment and then `DROP` that table when analysis is completed.

:::info Using pipeline mode in Databricks requires DROP permissions

Using pipeline mode in Databricks requires either granting DROP permissions to the Databricks account that GrowthBook uses, or leaving many tables in your schema you have to manually delete later! For this reason we strongly recommend a standalone schema for GrowthBook to use to write tables to.

:::

1. (strongly recommended, but optional) Create a dedicated schema to which GrowthBook will write temporary tables. This will keep your data warehouse clean and ensure that we are only writing to and dropping from a dedicated space.
2. Grant permissions to your user account or service principal that already has read permission in your warehouse. That user/service principle will need to be able to `USE SCHEMA`, `CREATE TABLE`, `DROP TABLE`, and to `SELECT` and `EXECUTE` in the schema.
3. Navigate to your Databricks Data Source in GrowthBook and scroll down to "Data Pipeline Settings"
4. Click "Edit" and enable pipeline mode, set the destination schema to your new dedicated GrowthBook schema from step 1, and whether you want the table to be deleted (we recommend you leave this setting on as we will not re-use these tables at a later date). If this setting is off, you'll need to manually delete the tables that GrowthBook creates.
