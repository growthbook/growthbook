---
title: CUPEDps
description: Regression Adjustment (CUPED) + post-stratification
sidebar_label: CUPED + post-stratification
slug: cuped
---

import MaxWidthImage from "@site/src/components/MaxWidthImage";

# CUPED + post-stratification

:::note

CUPED + post-stratification is a premium feature.

:::

GrowthBook aims to unlock high-velocity, enterprise-scale experimentation. CUPEDps (Controlled-experiment Using Pre-Experiment Data + post-stratification) is one way to increase the velocity of experimentation by reducing the uncertainty in estimates of experiment uplift.

### Why use CUPEDps?

CUPEDps decreases the variance of experiment uplift, increasing the accuracy of your experimental results and therefore the speed at which you can see the effects of an experiment. In the right conditions, CUPEDps can equate to getting 20% or more traffic during your experiment!

- In 2016, Netflix reported that CUPED reduced variance by roughly ~40% for some key engagement metrics ([source](https://www.kdd.org/kdd2016/papers/files/adp0945-xieA.pdf)).
- In 2022, Microsoft reported that, for one product team, CUPED was akin to adding 20% more traffic to analysis of a majority of metrics ([source](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/deep-dive-into-variance-reduction/)).

/* ### How does it work?

CUPEDps leverages both regression adjustment and post-stratification to improve the accuracy of your experiment results.  Suppose revenue is your key metric, and you have historical revenue data on your users.  You know that revenue spend varies across country (e.g. US vs. UK), and historical revenue spend is correlated with future revenue spend.  
In an ideal experiment, you would: 1) bucket users by country; 2) create pairs of users within each bucket that have the same revenue spend; and 3) randomly assign one user in each pair to the treatment and the other to the control.  Balancing helps ensure the treatment and control groups are similar before the experiment starts, and makes it easier to detect post-randomization differences.  
In online experimentation it is often infeasible to balance when randomizing. 

CUPEDps is a statistical technique that permits you to answer the question, "If I had run the ideal experiment, what would the lift be?"
Regression adjustment helps balance users with respect to their pre-randomization revenue spend.
Stratifying by country post-randomization (i.e., post-stratifying) helps balance users by country.  

Regression adjustment in general uses data correlated with a metric to reduce our uncertainty about that metric and the statistics we compute. Any data correlated with the metric of interest, but uncorrelated with variation assignment can be used. GrowthBook (and many others) use pre-experiment data, as metrics collected before an experiment will not be affected by said experiment. In fact, the very name CUPED embeds this concept of using pre-experiment data. Using this pre-experiment data, we can fit a simple model to predict our outcome metric and use those predictions to adjust our metric. The adjusted metric then tends to have lower variance.

The more correlated the pre-experiment data is with your metric of interest, the more variance reduction you can achieve. For example, the following plot demonstrates the difference in the distribution of a metric before adjustment ("Normal") and after adjustment ("Adjusted"). In both panels, the green, adjusted metric is distributed less widely (e.g. it is more tightly spaced out around the mean). However, the adjusted distribution is even tighter in the right plot, showing that variance reduction will be greater the more correlated your pre-experiment data is with your post-experiment data.

<MaxWidthImage maxWidth={600}>
  ![Variance Reduction by Correlation](/images/statistics/cuped-corr.png)
</MaxWidthImage>

In simpler terms, if we know a particular user tends to buy a lot of products on your website before you launch an experiment, we can use that information to understand whether purchase behavior after an experiment is driven by that customer's innate tendency to buy a lot of products or whether it was due to the experiment.

The concept of regression adjustment has been around for a long time, but you should feel free to read more in the original CUPED paper ([Deng et al. 2013](https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf)), a more general purpose paper on the underpinnings of regression adjustment from a sampling perspective ([Lin 2013](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Agnostic-notes-on-regression-adjustments-to-experimental-data--Reexamining/10.1214/12-AOAS583.full)), and any of the many blog posts on the topic (e.g. [Booking.com](https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d), [Microsoft](https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/deep-dive-into-variance-reduction/)).

Regression adjustment combined with post-stratification can bring additional accuracy gains in two ways. 
First, instead of running CUPED across all countries simultaneously, you can estimate country-specific lifts by running CUPED for each country individually.
Estimating country-specific relationships in pre-experiment revenue and post-experiment revenue can lead to additional variance reduction when these relationships vary by country (which they often do). 
Second, we estimate average lift across all users by weighting the country-specific lifts.  
The weights are the user proportions in each country. 
If there is an imbalance in the observed proportion of treatment users in a country compared to the variation split, then post-stratification will adjust the lift estimate for that country to account for the imbalance.




## CUPEDps implementation

GrowthBook takes a transparent, simple approach to regression adjustment.

For each metric you analyze, we use the metric itself from the pre-exposure period as the correlated data. This tends to be very powerful for metrics that are frequently produced by users (e.g. engagement measures), but can be less powerful if your metric is rare, or if you are measuring behavior for new users. In general, CUPED is more powerful the more you know about your units of interest and the longer they have been able to generate the metric that you are analyzing as a part of your experiment.

We then use the standard CUPED estimator for each variation mean,

$$
\bar{Y}_{adjusted} = \bar{Y} - \theta * \bar{X}
$$

where $\bar{Y}$ is the post-exposure metric average, $\bar{X}$ is the pre-exposure metric average, and $\theta$ is essentially a regression coefficient from a regression of the post-experiment data on the pre-experiment data (pooled across both the control and treatment variation of interest), $\theta = \text{Cov}(Y, X) / \text{Var}(X)$. We describe CUPED for ratio metrics and how to estimate uncertainty [here](/statistics/cuped-technical).

As discussed above, we could use any correlated data instead of $X$. For example, we could use some model that includes all pre-exposure metrics added to your experiment, or auxiliary dimension information you have configured per user. However, one downside of these approaches is that your results for metric A will depend on whether or not you add a metric B to your experiment and our analysis pipeline would lose its modularity, where each metric can be processed in parallel. Nonetheless, we anticipate continuing to build methods that leverage additional data to improve variance reduction from CUPED.

Post-stratification details are described [here](/statistics/post-stratification).

### Regression Adjustment lookback window

GrowthBook defaults to using 14 days of pre-exposure data, but this is customizable at the organization, metric, and metric-experiment level.

Why use a longer lookback window?

- A longer window can be better if your metric is low frequency and a longer window is needed to capture meaningful user behavior that will be correlated across the pre- and post-exposure time periods

Why use a shorter lookback window?

- Shorter lookback windows will yield more performant queries (fewer days to scan from your metric source)
- Behavior in the days just before a user enters an experiment is likely to be more highly correlated with behavior during an experiment, as users change over time. Of course, this is mostly true for metrics that are observed at a higher frequency (e.g. simple engagement metrics).

### Availability

CUPEDps works for all metrics except for:

- Legacy ratio metrics, or non query-optimized ratio [Fact Metrics](/app/metrics#fact-tables)
- Ratio metrics that are the goal metric for a [Bandit](/bandits/overview)
- Quantile metrics
- Metrics with custom user value aggregations
- Metrics from a MixPanel data sources

## Configuring CUPEDps

### Organization-level settings

You can turn CUPEDps on for all analyses under Settings â†’ General. CUPEDps can be turned on or off by default for all analyses and you can set the default number of days to use for a lookback window. This setting will set the default for all of your metrics, which will then flow through to all analyses that use those metrics.

<MaxWidthImage maxWidth={800} border>
  ![Organization-level CUPED Setting](/images/statistics/cuped-org.png)
</MaxWidthImage>

### Metric-level settings

You can also override these organization-level defaults at the Metric level. When creating or editing a Metric, go to the "Behavior" panel, click "Show advanced options" and scroll to the bottom. From there, you will see the following settings. These settings will allow you to disable CUPED for a metric, even if it is set at the organization or experiment level.

<MaxWidthImage maxWidth={700}>
  ![Metric-level CUPED Setting](/images/statistics/cuped-metric.png)
</MaxWidthImage>

You might want to disable CUPEDps for a particular metric if that metric never collects values for a user before they enter an experiment. You might want to adjust metric-specific lookback windows for any of the reasons listed in the section above.

Finally, you could also, if you wanted, override these metric-level settings for a particular experiment using metric overrides on the experiment page.

### Experiment settings and results

By default, each experiment will use your organization-level defaults, unless they are overridden by a metric. However, you can always toggle CUPED on or off using the toggle added to the top of the results table. You need to re-run your analysis if you change this setting.
If the toggle is On, then CUPEDps will be applied for all metrics, excluding the cases described above (e.g., legacy ratio metrics) and exceptions like metric overrides.

The following screenshot shows the results with CUPEDps on.
However, you can see that there is an icon showing that CUPEDps is disabled for revenue_p50 (because it is a quantile metric).
So even if CUPEDps is toggled on, we will always show you any metrics for which GrowthBook did not use CUPEDps using that small CUPEDps icon with a red x.

<MaxWidthImage maxWidth={800}>
  ![Experiment Results with CUPED toggle](/images/statistics/cuped-results-ratio.png)
</MaxWidthImage>

The screenshot below shows the same data with CUPEDps toggled off.
Note that the interval widths for the CUPEDps-enabled metrics are now much bigger, while the interval width for revenue_p50 is the same.
CUPEDps increases the accuracy of your estimates.

<MaxWidthImage maxWidth={800}>
  ![Experiment Results with CUPED toggle](/images/statistics/cuped-results-ratio-off.png)
</MaxWidthImage> 
