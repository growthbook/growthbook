---
title: Understanding Bandit Results 
slug: /bandits/results
---

## Leaderboard

The leaderboard is the best tool for comparing variations with respect to the decision metric.
In the example below, the decision metric is revenue.

<img
  src="/images/bandit_leaderboard_1.png"
  alt="Bandit Leaderboard"
  style={{ width: 800, margin: "0 auto" }}
/>
Each row of the leaderboard contains information about one of the five variations. 
The **Users** column on the leaderboard indicates the number of users per variation in the experiment. 
Note that the number of users is vastly unequal across variations, indicating that the bandit has been favoring some variations over others.
The second column displays the variation mean for the decision metric.
The chart to the right of the columns displays credible intervals for mean revenue for each metric.  
The black vertical lines indicate the estimated mean revenue for each variation and are equal to the values in the **Mean** column.
The colored bands around the black lines represent 95% credible intervals for mean revenue by variation.
The leaderboard helps visualize how user traffic allocations connect to the variation means. 
Variation 2 has the smallest number of users (2,575) and the lowest mean revenue ($31.95).
The other variations are roughly similar in terms of mean revenue, with control being slightly ahead.

## Graphs of Performance Over Time

Below the leaderboard are 3 different time series graphs to help you understand how the bandit evolved over time.

The first graph shows the cumulative means of the decision metric over time.

<img
  src="/images/cumulative_means.png"
  alt="Cumulative means"
  style={{ width: 600, margin: "0 auto" }}
/>
On all graphs shown, the vertical lines correspond to times when the variation weights were updated.
The overlap in the credible intervals in the leaderboard above causes mingling of the lines here. 
The second graph shows how the probability of each variation being the best changes over time.
<img
  src="/images/prob_of_winning.png"
  alt="Probability of winning"
  style={{ width: 600, margin: "0 auto" }}
/>
The volume of color on the graph corresponds to the probability that each variation is the best.    
At the beginning of the experiment during the exploration phase, each variation had an equal chance of being the best.
After the exploration phase, Variation 3 (in red) had the highest probability of being the best, and received the most traffic.  
Control and Variation 2 also had high probabilities of being the best, but received less traffic.
Variations 1 and 4 had low probabilities of being the best and received the least traffic.
The third graph shows the variation weights over time. 
<img
  src="/images/variation_weights.png"
  alt="Variation weights"
  style={{ width: 600, margin: "0 auto" }}
/>
As in the time series plot above displaying probability of being the best, variations with larger variation weights have a larger proportion of color on the graph. 
The variation weight for a period is equal to the probability of being the best arm, unless an arm has less than a 1% chance of being the best, in which case it is set to 1% and all of the other weights are recalibrated.
By mousing over the graph, the tool tip will display the variation weight and number of users for each variation at that time.  
This tooltip also works for the probability of winning graph.

## Explore Tab

While bandits suffer from biased results, it may sometimes be of interest to look at differences in variations, as in a standard experiment.
The explore tab shows standard experiment results for the decision metric and for any secondary or guardrail metrics.
Results on the explore tab are interpreted in the same way as in a standard experiment.
Again, we caution that these results may be biased, and should be interpreted with caution.

## Debugging Issues

[Sticky bucketing](/app/sticky-bucketing) is a feature that ensures that users are consistently assigned to the same variation throughout the experiment.
This is important for bandits, as it ensures that the same user is not assigned to different variations over time.
Sticky bucketing helps avoid multiple exposures, which can bias not only the variation estimates, but also the variation weights.
Sticky bucketing is enforced at the device id level.
If a user logs into multiple devices, each device will count as a separate experimental unit, and can trigger a multiple exposures warning.
If you are receiving multiple exposures warnings, you may want to check if multiple devices are mapped to the same user. Additionally, you want to ensure you have [correctly set up sticky bucketing](/bandits/config#prerequisite-sticky-bucketing).
